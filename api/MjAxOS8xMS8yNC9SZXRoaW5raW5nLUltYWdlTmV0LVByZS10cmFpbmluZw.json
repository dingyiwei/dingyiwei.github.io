{"title":"Notes of 「Rethinking ImageNet Pre-training」","date":"2019-11-23T16:00:00.000Z","date_formatted":{"ll":"Nov 24, 2019","L":"11/24/2019","MM-DD":"11-24"},"thumbnail":"2019/11/24/Rethinking-ImageNet-Pre-training/imagenet.jpg","link":"2019/11/24/Rethinking-ImageNet-Pre-training","tags":["深度学习","预训练"],"updated":"2020-05-28T20:04:06.337Z","content":"<p>paper: <a href=\"https://arxiv.org/pdf/1811.08883.pdf\" target=\"_blank\">https://arxiv.org/pdf/1811.08883.pdf</a></p>\n<p>First of all, let’s take a look at the interesting conclusions:</p>\n<h2 id=\"conclusion\">Conclusion<a href=\"#conclusion\" title=\"Conclusion\"></a></h2><ul><li>Training from scratch on target tasks is possible without architectural changes.</li>\n<li>Training from scratch requires more iterations to sufﬁciently converge.</li>\n<li>Training from scratch can be no worse than its ImageNet pre-training counterparts under many circumstances, down to as few as 10k COCO images.</li>\n<li>ImageNet pre-training speeds up convergence on the target task.</li>\n<li>ImageNet pre-training does not necessarily help reduce overfitting unless we enter a very small data regime.</li>\n<li>ImageNet pre-training helps less if the target task is more sensitive to localization than classiﬁcation.</li>\n</ul><h2 id=\"existing-paradigm\">Existing paradigm<a href=\"#existing-paradigm\" title=\"Existing paradigm\"></a></h2><p>Pre-train models using large-scale data (<em>e.g.</em>, ImageNet) and then fine-tune the models on target tasks that often have less training data. However, the improvements for object detection are <strong>small</strong> and <strong>scale poorly</strong> with the pre-training dataset size.</p>\n<h2 id=\"methodology\">Methodology<a href=\"#methodology\" title=\"Methodology\"></a></h2><p>In the paper, He <em>et al.</em> designs experiments to train typical architectures from scratch under <strong>minimal modifications</strong>. Two modifications are necessary, model normalization and training length.</p>\n<h3 id=\"normalization\">Normalization<a href=\"#normalization\" title=\"Normalization\"></a></h3><p>Successful forms of normalization include <strong>normalized parameter initialization</strong> and <strong>activation normalization layers</strong>.</p>\n<p>Batching Normalization (BN) partially makes training detectors from scratch difficult. Training with high resolution inputs reduces batch sizes, and small batch sizes severely degrade the accuracy (unstable). In pre-training, this can be circumvented by freezing BN layers to adopt the pre-training batch statistics.</p>\n<p>Two normalization strategies help relieve the small batch issue:</p>\n<ul><li><strong>Group Normalization (GN)</strong>: GN performs computation that is independent of the batch dimension. GN’s accuracy is insensitive to batch sizes.</li>\n<li><strong>Synchronized Batch Normalization (SyncBN)</strong>: this is an implementation of BN with batch statistics computed across multiple devices. This increases the effective batch size for BN when using many GPUs, which avoid small batches.</li>\n</ul><p>Experiments show that both GN and SyncBN can enable detection models to train from scratch.</p>\n<h3 id=\"convergence\">Convergence<a href=\"#convergence\" title=\"Convergence\"></a></h3><p>Models trained from scratch must be trained for longer than fine-tuning schedules since:</p>\n<ul><li>pre-training models have learned much from large-scale data with hundreds iteration</li>\n<li>learned low-level features (<em>e.g.</em>, edges, textures) do not need to be re-learned during fine-tuning</li>\n<li>when training from scratch, the model has to learn low- and high-level semantics, so more iterations may be necessary for it to converge well.</li>\n</ul><img src=\"/2019/11/24/Rethinking-ImageNet-Pre-training/training_data.png\" class=\"\" title=\"Training data\"><p>This suggests that a sufficiently large number of total samples are required for the models trained from random initialization to converge well.</p>\n<h2 id=\"experiments\">Experiments<a href=\"#experiments\" title=\"Experiments\"></a></h2><h3 id=\"settings\">Settings<a href=\"#settings\" title=\"Settings\"></a></h3><p><strong>Architecture</strong>: Mask R-CNN with ResNet or ResNeXt puls Feature Pyramid Network backbones.</p>\n<p><strong>Learning rate scheduling</strong>: 1× = 90k iterations. Experiments apply 2× to 6×. Learning rates are reduced by 10× in the last 60k and last 20k iterations respectively.</p>\n<p><strong>Hyper parameters</strong>:</p>\n<ul><li>the initial learning rate: 0.02</li>\n<li>the weight decay: 0.0001</li>\n<li>the momentum 0.9</li>\n<li>8 GPUs</li>\n<li>synchronized SGD</li>\n<li>batch size: 2</li>\n</ul><h3 id=\"results\">Results<a href=\"#results\" title=\"Results\"></a></h3><h3 id=\"with-sufficient-samples\">With sufficient samples<a href=\"#with-sufficient-samples\" title=\"With sufficient samples\"></a></h3><p>Data for experiments is COCO train2017 (~118k images) and COCO val2017 (~5k images).</p>\n<img src=\"/2019/11/24/Rethinking-ImageNet-Pre-training/gn.png\" class=\"\" title=\"GroupNormalization\"><img src=\"/2019/11/24/Rethinking-ImageNet-Pre-training/syncbn.png\" class=\"\" title=\"SyncBN\"><img src=\"/2019/11/24/Rethinking-ImageNet-Pre-training/comparison.png\" class=\"\" title=\"Comparison\"><p>There are a lot of extra results in the paper.</p>\n<h3 id=\"with-less-data\">With less data<a href=\"#with-less-data\" title=\"With less data\"></a></h3><p>With substantially less data (<em>e.g.</em>, ~1/10 of COCO), models trained from scratch are <strong>no worse</strong> than their counterparts that are pre-trained.</p>\n<img src=\"/2019/11/24/Rethinking-ImageNet-Pre-training/less_data.png\" class=\"\" title=\"Less data\"><p>Training from scratch with too less data (<em>e.g.</em>, ~1k images) will result in overfitting and cannot catch up to the pre-training results.</p>\n","prev":{"title":"Notes of 「Learning Unsupervised Video Object Segmentation through Visual Attention」","link":"2019/12/17/Learning-Unsupervised-Video-Object-Segmentation-through-Visual-Attention"},"next":{"title":"Notes of 「Digging into self-supervised monocular depth estimation」","link":"2019/11/14/Digging-into-self-supervised-monocular-depth-estimation"},"plink":"http://yoursite.com/2019/11/24/Rethinking-ImageNet-Pre-training/","toc":[{"id":"conclusion","title":"Conclusion","index":"1"},{"id":"existing-paradigm","title":"Existing paradigm","index":"2"},{"id":"methodology","title":"Methodology","index":"3","children":[{"id":"normalization","title":"Normalization","index":"3.1"},{"id":"convergence","title":"Convergence","index":"3.2"}]},{"id":"experiments","title":"Experiments","index":"4","children":[{"id":"settings","title":"Settings","index":"4.1"},{"id":"results","title":"Results","index":"4.2"},{"id":"with-sufficient-samples","title":"With sufficient samples","index":"4.3"},{"id":"with-less-data","title":"With less data","index":"4.4"}]}]}